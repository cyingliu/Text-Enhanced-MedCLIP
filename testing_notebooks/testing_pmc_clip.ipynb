{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a787fd56-c567-4703-a522-eb8a0c570f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from argparse import Namespace\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from torchvision.transforms import Normalize, Compose, RandomResizedCrop, InterpolationMode, ToTensor, Resize, CenterCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738d2d37-9ff4-46c0-bb1f-9dd74b02d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ebf8c50-45aa-4bb2-8bff-5506868cb5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630df153-4924-4a31-83bb-b0734724e401",
   "metadata": {},
   "source": [
    "## Check provided checkpoint\n",
    "Reference: https://huggingface.co/datasets/axiong/pmc_oa_beta/blob/main/checkpoint.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef5cf28-f609-45db-b66d-76e3cbbd58a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/pmc_clip/checkpoint.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7ee8d1-cba2-48ae-b8cb-6e14286c463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = checkpoint[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474185f-7e3e-4cd2-a56a-933c60d5e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional key in checkpoint\n",
    "# module.text_encoder.embeddings.position_ids => delete this key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d92d16be-6477-4383-81c8-6917d9903168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict[\"module.text_encoder.embeddings.position_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b07436e-94af-423f-8d84-afb19eeb1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in state_dict.keys():\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46af45a-e01e-456e-8c0d-73d6cf96634d",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9e3038-6ea9-44b8-85e9-3c27bd8a595e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_dim': 768,\n",
       " 'clip_model': 'PMC_CLIP',\n",
       " 'vision_cfg': {'image_size': 224,\n",
       "  'layers': [3, 4, 6, 3],\n",
       "  'width': 64,\n",
       "  'patch_size': None},\n",
       " 'text_cfg': {'context_length': 77,\n",
       "  'vocab_size': 30522,\n",
       "  'width': 768,\n",
       "  'heads': 8,\n",
       "  'layers': 12,\n",
       "  'fusion_layers': 4,\n",
       "  'bert_model_name': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path = \"Text_Enhanced_MedCLIP/pmc_clip/model_configs/RN50_fusion4.json\"\n",
    "model_config = json.load(open(config_path))\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218ff406-cdb4-451c-aac6-b201ab6793e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Text_Enhanced_MedCLIP.pmc_clip.model import PMC_CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59204b0f-5fc0-49f5-88b8-a9e4746d821d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PMC_CLIP'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = dict(bert_model_name=model_config['text_cfg']['bert_model_name'],\n",
    "            device=device,\n",
    "            mlm=True)\n",
    "args = Namespace(**args)\n",
    "model_config[\"args\"] = args\n",
    "model_config.pop(\"clip_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8ae3838-e9e5-4ae0-a9cb-762d7198fdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init3\n"
     ]
    }
   ],
   "source": [
    "model = PMC_CLIP(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ec4905-af0e-487e-95cf-3fa118820fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f22ecac-d87d-4e6c-b3db-6d7dee81f7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = {k[len('module.'):]: v for k, v in state_dict.items()}\n",
    "if \"text_encoder.embeddings.position_ids\" in sd:\n",
    "    del sd[\"text_encoder.embeddings.position_ids\"]\n",
    "model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "466be860-5756-41d8-a71c-7ac2ef1035e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd[\"text_encoder.embeddings.position_ids\"] # tensor arrange 0-511"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a55b80-4b4a-429f-b9f1-608d9547c7db",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f1e8bb8-543e-414a-9e4d-9428c5c9a8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'question', 'answer'],\n",
       "        num_rows: 1793\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'question', 'answer'],\n",
       "        num_rows: 451\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a57015-2aa8-4a5b-8938-13c487b70cd5",
   "metadata": {},
   "source": [
    "### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aeb53566-9957-43af-8c39-97109d560234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'question', 'answer'],\n",
       "        num_rows: 821\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['image', 'question', 'answer'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'question', 'answer'],\n",
       "        num_rows: 251\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_dataset = dataset[\"train\"].train_test_split(test_size=0.125, seed=123)\n",
    "train_val_test_dataset = DatasetDict({'train': train_val_dataset['train'],\n",
    "                                        'val': train_val_dataset['test'],\n",
    "                                        'test': dataset['test']})\n",
    "\n",
    "# binary task\n",
    "train_val_test_dataset = train_val_test_dataset.filter(lambda example: example[\"answer\"].lower() in (\"yes\", \"no\"))\n",
    "train_val_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9414d200-0581-46b2-8fb8-4f54d8890809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    \n",
    "    # binary\n",
    "    batch[\"labels\"] = [1 if answer.lower() == \"yes\" else 0 for answer in batch[\"answer\"]]\n",
    "    # mutli-class not implemented\n",
    "    \n",
    "    batch['bert_input'] = [question for question in batch['question']] # pmc-clip tokenize text inputs in the forward call\n",
    "    batch['bert_label'] = [question for question in batch['question']]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e33aadb-c982-4585-9986-1338cfb940a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'labels', 'bert_input', 'bert_label'],\n",
       "        num_rows: 821\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['image', 'labels', 'bert_input', 'bert_label'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'labels', 'bert_input', 'bert_label'],\n",
       "        num_rows: 251\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = train_val_test_dataset.map(preprocess, batched=True)\n",
    "processed_dataset = processed_dataset.remove_columns([\"question\", \"answer\"])\n",
    "# processed_dataset = processed_dataset.rename_column(\"image\", \"images\")\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a789882-0ede-46c6-8464-2632ea6f1bcd",
   "metadata": {},
   "source": [
    "### Image transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74b1692d-00c9-466f-8cc4-cf6da40f6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = model.visual.image_size\n",
    "crop_scale = 0.9 # follow pmc-clip pre-training\n",
    "mean = (0.48145466, 0.4578275, 0.40821073)  # OpenAI dataset mean\n",
    "std = (0.26862954, 0.26130258, 0.27577711)  # OpenAI dataset std\n",
    "\n",
    "def _convert_to_rgb(image):\n",
    "    return image.convert('RGB')\n",
    "\n",
    "train_image_transform =  Compose([\n",
    "                                RandomResizedCrop(image_size, scale=(crop_scale, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
    "                                _convert_to_rgb,\n",
    "                                ToTensor(),\n",
    "                                Normalize(mean=mean, std=std),\n",
    "                            ])\n",
    "test_image_transform = Compose([\n",
    "                            Resize(image_size, interpolation=InterpolationMode.BICUBIC),\n",
    "                            CenterCrop(image_size),\n",
    "                            _convert_to_rgb,\n",
    "                            ToTensor(),\n",
    "                            Normalize(mean=mean, std=std)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eaca84c1-2909-4564-aa6b-85a8fa715f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform(batch):\n",
    "    batch['image'] = [train_image_transform(img) for img in batch['image']]\n",
    "    return batch\n",
    "def test_transform(batch):\n",
    "    batch['image'] = [test_image_transform(img) for img in batch['image']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00358619-9809-406e-af93-30ce43efc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset['train'].set_transform(train_transform)\n",
    "processed_dataset['val'].set_transform(test_transform)\n",
    "processed_dataset['test'].set_transform(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e7d1b56-72a1-459e-a634-efa81670999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image [tensor([[[-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7777, -1.7923, -1.7923],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.4857, -1.7923, -1.7923],\n",
      "         ...,\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         [-1.7777, -1.7777, -1.7777,  ..., -1.7777, -1.7777, -1.7777],\n",
      "         [-1.7777, -1.7777, -1.7777,  ..., -1.7777, -1.7777, -1.7777]],\n",
      "\n",
      "        [[-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "         [-1.7521, -1.7521, -1.7521,  ..., -1.7371, -1.7521, -1.7521],\n",
      "         [-1.7521, -1.7521, -1.7521,  ..., -1.4369, -1.7521, -1.7521],\n",
      "         ...,\n",
      "         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "         [-1.7371, -1.7371, -1.7371,  ..., -1.7371, -1.7371, -1.7371],\n",
      "         [-1.7371, -1.7371, -1.7371,  ..., -1.7371, -1.7371, -1.7371]],\n",
      "\n",
      "        [[-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
      "         [-1.4802, -1.4802, -1.4802,  ..., -1.4660, -1.4802, -1.4802],\n",
      "         [-1.4802, -1.4802, -1.4802,  ..., -1.1816, -1.4802, -1.4802],\n",
      "         ...,\n",
      "         [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
      "         [-1.4660, -1.4660, -1.4660,  ..., -1.4660, -1.4660, -1.4660],\n",
      "         [-1.4660, -1.4660, -1.4660,  ..., -1.4660, -1.4660, -1.4660]]]), tensor([[[-1.7631, -1.7631, -1.7631,  ..., -1.7631, -1.7485, -1.7485],\n",
      "         [-1.7631, -1.7631, -1.7631,  ..., -1.7777, -1.7777, -1.7777],\n",
      "         [-1.7631, -1.7631, -1.7631,  ..., -1.7777, -1.7777, -1.7777],\n",
      "         ...,\n",
      "         [-1.7777, -1.7777, -1.7777,  ..., -1.7777, -1.7777, -1.7777],\n",
      "         [-1.7777, -1.7485, -1.7485,  ..., -1.7777, -1.7777, -1.7777],\n",
      "         [-1.7777, -1.7631, -1.7631,  ..., -1.7777, -1.7777, -1.7777]],\n",
      "\n",
      "        [[-1.7221, -1.7221, -1.7221,  ..., -1.7221, -1.7071, -1.7071],\n",
      "         [-1.7221, -1.7221, -1.7221,  ..., -1.7371, -1.7371, -1.7371],\n",
      "         [-1.7221, -1.7221, -1.7221,  ..., -1.7371, -1.7371, -1.7371],\n",
      "         ...,\n",
      "         [-1.7371, -1.7371, -1.7371,  ..., -1.7371, -1.7371, -1.7371],\n",
      "         [-1.7371, -1.7071, -1.7071,  ..., -1.7371, -1.7371, -1.7371],\n",
      "         [-1.7371, -1.7221, -1.7221,  ..., -1.7371, -1.7371, -1.7371]],\n",
      "\n",
      "        [[-1.4518, -1.4518, -1.4518,  ..., -1.4518, -1.4376, -1.4376],\n",
      "         [-1.4518, -1.4518, -1.4518,  ..., -1.4660, -1.4660, -1.4660],\n",
      "         [-1.4518, -1.4518, -1.4518,  ..., -1.4660, -1.4660, -1.4660],\n",
      "         ...,\n",
      "         [-1.4660, -1.4660, -1.4660,  ..., -1.4660, -1.4660, -1.4660],\n",
      "         [-1.4660, -1.4376, -1.4376,  ..., -1.4660, -1.4660, -1.4660],\n",
      "         [-1.4660, -1.4518, -1.4518,  ..., -1.4660, -1.4660, -1.4660]]])]\n",
      "labels [0, 1]\n",
      "bert_input ['is there evidence of large calcified lesions in the lung fields?', 'is there evidence of midlight shift of structures on this mri?']\n",
      "bert_label ['is there evidence of large calcified lesions in the lung fields?', 'is there evidence of midlight shift of structures on this mri?']\n"
     ]
    }
   ],
   "source": [
    "batch = processed_dataset['train'][:2]\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.shape)\n",
    "    else:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c62348a-9f71-4275-9f48-56eddf8f7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch key\n",
    "# images, bert_input, bert_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f721bec-b857-4557-b5f5-3c74bb152bdd",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43b5b6c6-5e06-44b2-ad7c-86adb88a6178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_images_and_label_data_collator(features: List[Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "        Collate images and label into tensors,\n",
    "        leave bert_input and bert_label as list of strings,\n",
    "        which will be tokenized and collated in PMC-CLIP\n",
    "    \"\"\"\n",
    "    first = features[0]\n",
    "    for k, v in first.items():\n",
    "        if k not in (\"bert_input\", \"bert_label\") and v is not None:\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                batch[k] = torch.stack([f[k] for f in features])\n",
    "            elif isinstance(v, np.ndarray):\n",
    "                batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n",
    "            else:\n",
    "                batch[k] = torch.tensor([f[k] for f in features])\n",
    "        else:\n",
    "            batch[k] = [f[k] for f in features]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f55404a-c691-4895-9b09-d58ed1d4e853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image torch.Size([4, 3, 224, 224])\n",
      "labels torch.Size([4])\n",
      "bert_input ['is there evidence of large calcified lesions in the lung fields?', 'is there evidence of midlight shift of structures on this mri?', 'are the colon walls thickened?', 'is there cardiac enlargement?']\n",
      "bert_label ['is there evidence of large calcified lesions in the lung fields?', 'is there evidence of midlight shift of structures on this mri?', 'are the colon walls thickened?', 'is there cardiac enlargement?']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(processed_dataset['train'], collate_fn=torch_images_and_label_data_collator, batch_size=4)\n",
    "data_iter = iter(data_loader)\n",
    "batch = next(data_iter)\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.shape)\n",
    "    else:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "190dbc17-ceb3-4ebc-bf77-3c10075b5803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_token_index: tensor([[0, 0],\n",
      "        [1, 0],\n",
      "        [2, 0],\n",
      "        [3, 0]])\n"
     ]
    }
   ],
   "source": [
    "# satisfy pmc-clip input format\n",
    "with torch.no_grad():\n",
    "    output = model(batch)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "068955b3-d130-4f3f-8fe4-ba621050dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2ddf99a-4d28-4358-9620-09bc15bec0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2182fb2b-acd5-4056-9c25-d3ab85ee0e54",
   "metadata": {},
   "source": [
    "## PMC-CLIP for VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c3b2753e-2dda-4928-8a5f-3e19819ae2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoProcessor, AutoTokenizer\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "\n",
    "from Text_Enhanced_MedCLIP.pmc_clip.model import PMC_CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3f5420ca-a83e-4425-abd0-20fe7c07ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMC_CLIPforVQA(nn.Module):\n",
    "    \"\"\"\n",
    "        Apply a fully connected network on the pooled_output of the fusion module\n",
    "        Predict a scalar score for binary/multi-class classification\n",
    "        \n",
    "        Inputs:\n",
    "            checkpoint_path: download checkpoint from https://huggingface.co/datasets/axiong/pmc_oa_beta/blob/main/checkpoint.pt\n",
    "            config_path: RN50_fusion4.json for the provided checkpoint, download configs from PMC-CLIP repo\n",
    "            text_model_path\n",
    "            num_labels\n",
    "            pool_type: \"average\" or \"cls\"\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_path, config_path, text_model_path=None, num_labels=2, pool_type=\"average\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        model_config = json.load(open(config_path))\n",
    "        args = dict(bert_model_name=model_config['text_cfg']['bert_model_name'],\n",
    "                    device=device,\n",
    "                    mlm=True)\n",
    "        args = Namespace(**args)\n",
    "        model_config[\"args\"] = args\n",
    "        model_config.pop(\"clip_model\")\n",
    "        self.base_model = PMC_CLIP(**model_config)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "        sd = {k[len('module.'):]: v for k, v in state_dict.items()}\n",
    "        if \"text_encoder.embeddings.position_ids\" in sd:\n",
    "            del sd[\"text_encoder.embeddings.position_ids\"]\n",
    "        self.base_model.load_state_dict(sd)\n",
    "        self.cls_id = self.base_model.tokenizer.cls_token_id\n",
    "        \n",
    "        if text_model_path:\n",
    "            print(\"Load text model weight from:\", text_model_path)\n",
    "            text_model_dict = torch.load(text_model_path)\n",
    "            text_model_dict = {k: v for k, v in text_model_dict.items() if k.startswith(\"text_model\")}\n",
    "            base_model_dict = self.base_model.state_dict()\n",
    "            base_model_dict.update(text_model_dict)\n",
    "            self.base_model.load_state_dict(base_model_dict)\n",
    "        \n",
    "        self.num_labels = num_labels\n",
    "        projection_dim = self.base_model.transformer_width # 768\n",
    "        output_dim = 1 if num_labels == 2 else num_labels # scalar output for binary classification\n",
    "        self.MLP = nn.Sequential(nn.Linear(projection_dim, 512),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(512, 128),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(128, output_dim))\n",
    "        \n",
    "        self.pool_type = pool_type\n",
    "        \n",
    "    def forward(self,\n",
    "                bert_input: Optional[List[str]] = None,\n",
    "                bert_label: Optional[List[str]] = None,\n",
    "                image: Optional[torch.FloatTensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "\n",
    "        image_features = self.base_model.encode_image(image)\n",
    "        image_features = F.normalize(image_features['image_features'], dim=-1)  # (bs, 768)\n",
    "\n",
    "        batch = dict(bert_input=bert_input, bert_label=bert_label)\n",
    "        text_output = self.base_model.encode_text(batch, image_features)\n",
    "\n",
    "        fusion_features = text_output[\"fusion_features\"] # (bs, 79, 768)\n",
    "\n",
    "        if self.pool_type == \"average\":\n",
    "            pooled_feature = fusion_features.mean(dim=1) # (bs, 768)\n",
    "        elif self.pool_type == \"cls\":\n",
    "            last_token_index = torch.nonzero((text_output[\"encoded_input\"]['input_ids'] == self.cls_id).squeeze())\n",
    "            pooled_feature = fusion_features[torch.arange(fusion_features.shape[0]), last_token_index[:, 1]] # the 0-index of each row\n",
    "\n",
    "        print(\"pooled_feature:\", pooled_feature.shape)\n",
    "        logits = self.MLP(pooled_feature).squeeze() # (N,) or (N, C)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.num_labels == 2:\n",
    "                # binary classification\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits.squeeze(), labels.squeeze().float())\n",
    "            else:\n",
    "                # multi-class classification\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return ImageClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03a5f979-7276-486b-a8aa-c5714718569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/pmc_clip/checkpoint.pt\"\n",
    "config_path = \"Text_Enhanced_MedCLIP/pmc_clip/model_configs/RN50_fusion4.json\"\n",
    "model = PMC_CLIPforVQA(checkpoint_path, config_path, pool_type=\"cls\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "502e7f8c-d0ee-48dc-82bc-0d053df4e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a95a26ff-3ebc-4d84-a13b-f1d126da789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pooled_feature: torch.Size([4, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(2.0616, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0969, -0.3604, -0.2607,  0.5581, -0.5485],\n",
       "        [ 0.3941, -0.2613,  0.0454,  0.6127, -0.7037],\n",
       "        [ 0.6128, -0.4206, -0.1830,  0.7622, -0.0140],\n",
       "        [ 0.3079, -0.8749, -0.1913,  0.5312, -0.4160]],\n",
       "       grad_fn=<SqueezeBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24125acb-56fc-43a3-90e7-80c633d30cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
