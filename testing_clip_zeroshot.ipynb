{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87fb2b23-8b18-455a-be5e-02dbbadf01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoModel, AutoTokenizer,  AutoImageProcessor, CLIPImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08846d55-3f6b-4228-bae7-59090d98c090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c02af25-ff31-48c6-9dbf-acd63fbf0397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'question', 'answer'],\n",
       "        num_rows: 821\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['image', 'question', 'answer'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'question', 'answer'],\n",
       "        num_rows: 251\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
    "train_val_dataset = dataset[\"train\"].train_test_split(test_size=0.125, seed=123)\n",
    "train_val_test_dataset = DatasetDict({'train': train_val_dataset['train'],\n",
    "                                      'val': train_val_dataset['test'],\n",
    "                                      'test': dataset['test']})\n",
    "close_ended_train_val_test_dataset = train_val_test_dataset.filter(lambda example: example[\"answer\"].lower() in (\"yes\", \"no\"))\n",
    "close_ended_train_val_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de80e11b-7ff2-4eae-980a-fd0e03a5b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is there evidence of large calcified lesions in the lung fields?\n",
      "is there evidence of midlight shift of structures on this mri?\n",
      "are the colon walls thickened?\n",
      "is there cardiac enlargement?\n",
      "is there a pneumothorax?\n",
      "is there a mass demonstrated?\n",
      "is the jejunal wall enlarged?\n",
      "is there an aortic aneurysm?\n",
      "is the liver normal?\n",
      "are the sulci visible in this image?\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(close_ended_train_val_test_dataset[\"train\"][i][\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58166a0-619e-4b9d-8235-4423157f2d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/deep/u/ying1029/anaconda3/envs/cs231n/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/deep/u/ying1029/anaconda3/envs/cs231n/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a0a4f8-f8f5-4574-bb4a-d9a6dd4408fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = close_ended_train_val_test_dataset[\"train\"][0][\"question\"]\n",
    "answer = close_ended_train_val_test_dataset[\"train\"][0][\"answer\"]\n",
    "image = close_ended_train_val_test_dataset[\"train\"][0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc5a940f-2d27-442a-b2b8-60f56edd84eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is there evidence of large calcified lesions in the lung fields? Yes.',\n",
       " 'is there evidence of large calcified lesions in the lung fields? No.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = lambda question: [question + \" Yes.\", question + \" No.\"]\n",
    "template(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3571f37a-7fdc-48ee-bef5-496f69ef5bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no no\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(text=template(question), images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "pred_id = probs.argmax().item()\n",
    "prediction = \"yes\" if pred_id == 0 else \"no\"\n",
    "print(answer, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f1c0fde-7e5d-4600-a9fb-7b400ac443bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 821/821 [00:12<00:00, 63.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 156.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 251/251 [00:01<00:00, 190.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for split in close_ended_train_val_test_dataset:\n",
    "    print(split)\n",
    "    for i in tqdm(range(len(close_ended_train_val_test_dataset[split]))):\n",
    "        answer = close_ended_train_val_test_dataset[split][i][\"answer\"]\n",
    "        assert answer == \"yes\" or answer == \"no\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77d6fc-98cc-4b9e-b484-024560a83c39",
   "metadata": {},
   "source": [
    "## Zero-shot inference on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e9f6b7b-0391-47f9-8626-611cfef36cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_eval(dataset, template):\n",
    "    correct = 0\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        question = dataset[i][\"question\"]\n",
    "        answer = dataset[i][\"answer\"]\n",
    "        image = dataset[i][\"image\"]\n",
    "\n",
    "\n",
    "        inputs = processor(text=template(question), images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "        probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "        pred_id = probs.argmax().item()\n",
    "        prediction = \"yes\" if pred_id == 0 else \"no\"\n",
    "        if answer == prediction:\n",
    "            correct += 1\n",
    "    return correct / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80878de5-12d8-4f46-ae82-223ac52e3cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 119/119 [00:18<00:00,  6.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48739495798319327"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = lambda question: [question + \" Yes.\", question + \" No.\"]\n",
    "zero_shot_eval(close_ended_train_val_test_dataset[\"val\"], template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23a8ccc3-b12d-4c07-b4dd-a85d8de83334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 119/119 [00:19<00:00,  6.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4369747899159664"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = lambda question: [f\"Q: {question} A: Yes.\", f\"Q: {question} A: No.\"]\n",
    "zero_shot_eval(close_ended_train_val_test_dataset[\"val\"], template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c3b18fc-8665-4c53-b847-5036a0aff949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 119/119 [00:19<00:00,  6.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = lambda question: [f\"The answer of the question: {question} is Yes\", \n",
    "                             f\"The answer of the question: {question} is No\"]\n",
    "zero_shot_eval(close_ended_train_val_test_dataset[\"val\"], template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c77ff3aa-49b2-4e90-903d-fe35bdd09850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 251/251 [00:40<00:00,  6.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.50199203187251"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = lambda question: [f\"The answer of the question: {question} is Yes\", \n",
    "                             f\"The answer of the question: {question} is No\"]\n",
    "zero_shot_eval(close_ended_train_val_test_dataset[\"test\"], template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49751f10-9c56-4d80-9a4b-42e7d9d6f566",
   "metadata": {},
   "source": [
    "## Zero-shot inference on general domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7fc9bb2-2b58-4abf-b1c5-8f31a9415151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_example(text, image):\n",
    "    inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "979785eb-90c5-4227-8a2b-bfee14762038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    " \n",
    "# Load the image\n",
    "image = Image.open(\"cat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24471307-8b93-4527-b1aa-73bebc115820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5688, 0.4312]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Is this a cat?\"\n",
    "template = lambda question: [f\"The answer of the question: {question} is Yes\", \n",
    "                             f\"The answer of the question: {question} is No\"]\n",
    "text = template(question)\n",
    "eval_one_example(text, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0741a864-3a54-48de-8748-46f13a360287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9855, 0.0145]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"This is a cat.\", \"This is a dog.\"]\n",
    "eval_one_example(text, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af2f17de-15ff-45df-9045-df84bb9846f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5849, 0.4151]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"This is a cat.\", \"This is not a cat.\"]\n",
    "eval_one_example(text, image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
